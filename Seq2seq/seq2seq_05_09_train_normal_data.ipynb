{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ppfhf60UpsXI"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#torch.set_default_tensor_type('torch.cuda.LongTensor')\n",
    "\n",
    "class Seq2seq(nn.Module):\n",
    "\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size, window_size, device):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(voc_size, embedding_size)\n",
    "        self.enc_lstm = nn.LSTM(embedding_size, hidden_size, num_layers=3,batch_first = True)\n",
    "        self.enc_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.dec_lstm = nn.LSTM(embedding_size, hidden_size, num_layers=3, batch_first = True)\n",
    "        self.dec_out = nn.Linear(hidden_size, voc_size)\n",
    "\n",
    "        self.start_of_string_token = 76\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.device=self.set_device(device)\n",
    "\n",
    "    def Encoder(self, x):\n",
    "        embedded = self.embedding(x) ## View not needed\n",
    "        output = embedded \n",
    "    \n",
    "        output, hidden =self.enc_lstm(output)\n",
    "    \n",
    "    #output, _ = self.enc_lstm(output)\n",
    "    #output = output[:,-1,:]\n",
    "    ## You could also take the hidden representation here and make that he output\n",
    "    ## But I think the important part is just that the input sequence is encoded into some representation\n",
    "    ## And this representation can either be the output of the last LSTM cell or the hidden state in that LSTM cell\n",
    "    #output = self.enc_out(output) ## Not necessary, can take output from lstm, or hidden output from lstm\n",
    "        return hidden\n",
    "\n",
    "    def Decoder(self, x, hidden):\n",
    "        output = self.embedding(x) ## View not necessary\n",
    "\n",
    "    #c0 = torch.zeros_like(hidden)\n",
    "        \n",
    "    #output, hidden = self.dec_lstm(output, (hidden, c0))\n",
    "        output, hidden = self.dec_lstm(output, hidden)\n",
    "        output = self.dec_out(output) ## No zeroth element\n",
    "        return output, hidden\n",
    "    \n",
    "    def set_device(self, gpu=-1):\n",
    "        if gpu!=-1 and torch.cuda.is_available():\n",
    "            device=torch.device('cuda: ' + str(gpu))\n",
    "        else:\n",
    "            device=torch.device('cpu')\n",
    "        return device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.to(self.device)\n",
    "        enc_hidden = self.Encoder(x)\n",
    "\n",
    "    #enc_hidden = torch.unsqueeze(enc_hidden, dim = 0)\n",
    "        enc_h = enc_hidden[0]\n",
    "        enc_cs = enc_hidden[1]\n",
    "\n",
    "    ## During training we can use the gold standard inputs\n",
    "    ## Create the start of string token\n",
    "        start_of_string_t = torch.tensor(x.shape[0]*[self.start_of_string_token]).view(1,-1).to(self.device) #.cuda()\n",
    "    \n",
    "    ## Concatenate it with the real inputs\n",
    "        x_start = torch.cat((start_of_string_t,x[:,:-1].t())).t().to(self.device)#.cuda()\n",
    "        x,_ = self.Decoder(x_start, (enc_h, enc_cs))\n",
    "        return x\n",
    "\n",
    "    \n",
    "  \n",
    "  ## When we predict then we do not want to feed the sequence to the decoder\n",
    "  ## But instead use the output sequentially\n",
    "    def predict(self, x):\n",
    "        x=x.to(self.device)\n",
    "        enc_hidden = self.Encoder(x)\n",
    "    #enc_hidden = torch.unsqueeze(enc_hidden, dim = 0)\n",
    "        enc_h=enc_hidden[0]\n",
    "        enc_c=enc_hidden[1]\n",
    "        x_start = torch.tensor([self.start_of_string_token]).view(1,-1).to(self.device) #.cuda()\n",
    "    \n",
    "        output = self.embedding(x_start)\n",
    "    #c0 = torch.zeros_like(enc_hidden)\n",
    "    #output, hidden = self.dec_lstm(output, (enc_hidden, c0))\n",
    "        output, hidden = self.dec_lstm(output, (enc_h, enc_c))\n",
    "        output = self.dec_out(output) ## No zeroth element\n",
    "\n",
    "        log_k = torch.argmax(output, dim = 2)\n",
    "    \n",
    "    ## Store all the obtained log_keys. Can be done in a more efficient way \n",
    "        hidden=enc_hidden\n",
    "        all_log_k = [log_k.item()]\n",
    "        for i in range(1, self.window_size):\n",
    "            output = self.embedding(log_k)\n",
    "            output, hidden = self.dec_lstm(output, hidden)\n",
    "            output = self.dec_out(output)\n",
    "            log_k = torch.argmax(output, dim = 2)\n",
    "            all_log_k.append(log_k.item())\n",
    "    \n",
    "        return all_log_k\n",
    "\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        x=x.to(self.device)\n",
    "        \n",
    "        # run sequence through encoder\n",
    "        enc_hidden = self.Encoder(x)\n",
    "\n",
    "        # save the hidden states for the first decoder LSTM cell\n",
    "        enc_h=enc_hidden[0]\n",
    "        enc_c=enc_hidden[1]\n",
    "        \n",
    "        # create the start of string tokens\n",
    "        start_of_string_t = torch.tensor(x.shape[0]*[self.start_of_string_token]).view(1,-1).to(self.device)\n",
    "        \n",
    "        for i in range(self.window_size):\n",
    "            \n",
    "          # take latest output as input\n",
    "          output=self.embedding(start_of_string_t)#[:,-1])\n",
    "          out, (enc_h, enc_c)= self.dec_lstm(output.unsqueeze(0), (enc_h, enc_c))\n",
    "        \n",
    "          out=self.dec_out(out)\n",
    "\n",
    "          start_of_string_t=torch.cat((start_of_string_t, torch.argmax(out, dim=2)[:,-1].view(-1,1)), dim=1)\n",
    "        \n",
    "        return start_of_string_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3Kn7Fd2mpsXa"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, encoder_series):\n",
    "        self.encoder_series=encoder_series\n",
    "        self.num_total_seqs=len(self.encoder_series)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_total_seqs\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        encoder_input=self.encoder_series.iloc[index]\n",
    "        \n",
    "        return torch.LongTensor(encoder_input) #.cuda()\n",
    "    \n",
    "def dataloader_(encoder_series, batch_size):\n",
    "    dataset=Dataset(encoder_series)\n",
    "    \n",
    "    data_loader=torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                           shuffle=False, drop_last=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wWUI7WCXpsXV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hdfs_train=pd.read_pickle('C:\\\\Users\\\\A373503\\\\Desktop\\\\Complete_4_models\\\\HDFS\\\\Transformer\\\\No_missing_windows_version\\\\train_normal_data_pytorch_05_17.pkl')\n",
    "\n",
    "word2id=pd.read_pickle('C:\\\\Users\\\\A373503\\\\Desktop\\\\Complete_4_models\\\\HDFS\\\\Data\\\\word2id_train_normal.pkl')\n",
    "#hdfs_test_anomalies=pd.read_pickle('anomalies_test_pytorch_autoencoder.pkl')\n",
    "#hhfs_test_normal=pd.read_pickle('normal_test_pytorch_autoencoder.pkl')\n",
    "\n",
    "## Below we are initializing the special tokens\n",
    "word2id['<unk>']=len(word2id)+1\n",
    "word2id['<sos>']=len(word2id)+1\n",
    "word2id['<pad>']=len(word2id)+1\n",
    "\n",
    "#df_train=df_train['Windows']\n",
    "\n",
    "## Specify device (gpu if available, otherwise cpu)\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Windows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 12, 3, 4, 3, 4, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 12, 3, 4, 3, 4, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[2, 12, 3, 4, 3, 4, 3, 4, 5, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[12, 3, 4, 3, 4, 3, 4, 5, 5, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[3, 4, 3, 4, 3, 4, 5, 5, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4688012</td>\n",
       "      <td>[9, 9, 9, 10, 6, 6, 6, 7, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4688013</td>\n",
       "      <td>[2, 2, 12, 2, 3, 4, 3, 4, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4688014</td>\n",
       "      <td>[2, 12, 2, 3, 4, 3, 4, 3, 4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4688015</td>\n",
       "      <td>[12, 2, 3, 4, 3, 4, 3, 4, 5, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4688016</td>\n",
       "      <td>[2, 3, 4, 3, 4, 3, 4, 5, 5, 14]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4688017 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Windows\n",
       "0        [2, 2, 2, 12, 3, 4, 3, 4, 3, 4]\n",
       "1        [2, 2, 12, 3, 4, 3, 4, 3, 4, 5]\n",
       "2        [2, 12, 3, 4, 3, 4, 3, 4, 5, 5]\n",
       "3        [12, 3, 4, 3, 4, 3, 4, 5, 5, 5]\n",
       "4         [3, 4, 3, 4, 3, 4, 5, 5, 5, 6]\n",
       "...                                  ...\n",
       "4688012  [9, 9, 9, 10, 6, 6, 6, 7, 7, 8]\n",
       "4688013  [2, 2, 12, 2, 3, 4, 3, 4, 3, 4]\n",
       "4688014  [2, 12, 2, 3, 4, 3, 4, 3, 4, 5]\n",
       "4688015  [12, 2, 3, 4, 3, 4, 3, 4, 5, 5]\n",
       "4688016  [2, 3, 4, 3, 4, 3, 4, 5, 5, 14]\n",
       "\n",
       "[4688017 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx=word2id['<pad>']\n",
    "trg_pad_idx=word2id['<pad>']\n",
    "src_vocab_size=len(word2id)\n",
    "trg_vocab_size=len(word2id)\n",
    "window_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blockid</th>\n",
       "      <th>Preprocess_to_log_lines</th>\n",
       "      <th>labels</th>\n",
       "      <th>Length</th>\n",
       "      <th>EventSequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>525055</td>\n",
       "      <td>blk_5604007695029308798</td>\n",
       "      <td>[_info_dfs_datanode_dataxceiver_receiving_bloc...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>[2, 2, 11, 2, 3, 4, 3, 4, 3, 4, 5, 5, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446838</td>\n",
       "      <td>blk_-8684220662358283959</td>\n",
       "      <td>[_info_dfs_datanode_dataxceiver_receiving_bloc...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>[2, 2, 11, 2, 3, 4, 3, 4, 3, 4, 5, 5, 5, 6, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146810</td>\n",
       "      <td>blk_1749152527273777138</td>\n",
       "      <td>[_info_dfs_datanode_dataxceiver_receiving_bloc...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>[2, 12, 2, 2, 3, 4, 3, 4, 3, 4, 5, 5, 5, 9, 9,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504536</td>\n",
       "      <td>blk_-7575024238572130397</td>\n",
       "      <td>[_info_dfs_datanode_dataxceiver_receiving_bloc...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>[2, 2, 11, 2, 3, 4, 3, 4, 3, 4, 5, 5, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411686</td>\n",
       "      <td>blk_-2768058268628005360</td>\n",
       "      <td>[_info_dfs_datanode_dataxceiver_receiving_bloc...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>[2, 2, 2, 12, 3, 4, 3, 4, 3, 4, 5, 5, 5, 6, 6,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Blockid  \\\n",
       "525055   blk_5604007695029308798   \n",
       "446838  blk_-8684220662358283959   \n",
       "146810   blk_1749152527273777138   \n",
       "504536  blk_-7575024238572130397   \n",
       "411686  blk_-2768058268628005360   \n",
       "\n",
       "                                  Preprocess_to_log_lines labels  Length  \\\n",
       "525055  [_info_dfs_datanode_dataxceiver_receiving_bloc...      0      13   \n",
       "446838  [_info_dfs_datanode_dataxceiver_receiving_bloc...      0      19   \n",
       "146810  [_info_dfs_datanode_dataxceiver_receiving_bloc...      0      25   \n",
       "504536  [_info_dfs_datanode_dataxceiver_receiving_bloc...      0      13   \n",
       "411686  [_info_dfs_datanode_dataxceiver_receiving_bloc...      0      19   \n",
       "\n",
       "                                            EventSequence  \n",
       "525055          [2, 2, 11, 2, 3, 4, 3, 4, 3, 4, 5, 5, 14]  \n",
       "446838  [2, 2, 11, 2, 3, 4, 3, 4, 3, 4, 5, 5, 5, 6, 6,...  \n",
       "146810  [2, 12, 2, 2, 3, 4, 3, 4, 3, 4, 5, 5, 5, 9, 9,...  \n",
       "504536          [2, 2, 11, 2, 3, 4, 3, 4, 3, 4, 5, 5, 14]  \n",
       "411686  [2, 2, 2, 12, 3, 4, 3, 4, 3, 4, 5, 5, 5, 6, 6,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_pickle('C:\\\\Users\\\\A373503\\\\Desktop\\\\Complete_4_models\\\\HDFS\\\\Transformer\\\\No_missing_windows_version\\\\transformer_normal_test.pkl')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [2, 2, 2, 12, 3, 4, 3, 4, 3, 4]\n",
       "1          [2, 2, 12, 3, 4, 3, 4, 3, 4, 5]\n",
       "2          [2, 12, 3, 4, 3, 4, 3, 4, 5, 5]\n",
       "3          [12, 3, 4, 3, 4, 3, 4, 5, 5, 5]\n",
       "4           [3, 4, 3, 4, 3, 4, 5, 5, 5, 6]\n",
       "                        ...               \n",
       "4688012    [9, 9, 9, 10, 6, 6, 6, 7, 7, 8]\n",
       "4688013    [2, 2, 12, 2, 3, 4, 3, 4, 3, 4]\n",
       "4688014    [2, 12, 2, 3, 4, 3, 4, 3, 4, 5]\n",
       "4688015    [12, 2, 3, 4, 3, 4, 3, 4, 5, 5]\n",
       "4688016    [2, 3, 4, 3, 4, 3, 4, 5, 5, 14]\n",
       "Name: Windows, Length: 4688017, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_train['Windows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=hdfs_train['Windows'].sample(frac = 0.01)\n",
    "train_loader=dataloader_(df_train, 128)\n",
    "\n",
    "\n",
    "#df_test=hdfs_train[3865473:]['Windows'].sample(frac = 0.01)\n",
    "\n",
    "#test_loader=dataloader_(df_test, len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CA3_GUyqpsXb",
    "outputId": "743b0017-97cd-4bc5-c876-9d20eb3392c7"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2b4273844fdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m#print(out)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#print(len(out))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-33db01e3a787>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m## Concatenate it with the real inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mx_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_of_string_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0menc_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_cs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-33db01e3a787>\u001b[0m in \u001b[0;36mDecoder\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## View not necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m#c0 = torch.zeros_like(hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m         return F.embedding(\n\u001b[0;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1916\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "voc_size = len(word2id)\n",
    "batch_size = 128\n",
    "window_size = 10\n",
    "#batches_in_epoch = 50\n",
    "embedding_size = 128\n",
    "hidden_size = 128\n",
    "num_epochs = 4\n",
    "\n",
    "seq2seq = Seq2seq(voc_size, embedding_size, hidden_size, window_size, device)\n",
    "#mod=my_auto.cuda()\n",
    "\n",
    "opt = torch.optim.SGD(seq2seq.parameters(), lr = 1e-1, momentum = 0.8)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "data_loader=dataloader_(df_train, batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    acc=0\n",
    "    for b in data_loader:\n",
    "        seq2seq.zero_grad()\n",
    "        x = b\n",
    "        y = x\n",
    "        out = seq2seq(x) #.cuda()\n",
    "        #print(out)\n",
    "        #print(len(out))\n",
    "        out = out.reshape(batch_size*window_size, -1)\n",
    "        targets = y.reshape(batch_size*window_size)\n",
    "        \n",
    "        #print(\"predicted:\")\n",
    "        #print(torch.argmax(out, dim=1))\n",
    "        #print(\"targets:\")\n",
    "        #print(targets)\n",
    "        #print(\"equal:\")\n",
    "        accuracy=torch.sum(torch.argmax(out, dim=1)==targets)/out.size(0) #((targets==torch.argmax(out, dim=1)).sum())/len(targets)\n",
    "        acc+=accuracy.item()\n",
    "        loss = loss_fn(out, targets)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    #with torch.no_grad():\n",
    "    #    sum_=0\n",
    "     #   samples=10\n",
    "      #  for d in range(samples):\n",
    "                #i=random.randint(0,len(data_not_trained_on)-1)\n",
    "       #     x_predict = torch.LongTensor(df_test.iloc[d]).view(1, -1).to(device) #.cuda()\n",
    "    \n",
    "            #if d%(samples/10)==0:\n",
    "            #    print('Predicted {} : Input {}'.format(seq2seq.predict(x_predict), x_predict.tolist()[0]))\n",
    "        #    sum_+=sum([a == b for a, b in zip(seq2seq.predict(x_predict),x_predict.tolist()[0])])/len(x_predict.tolist()[0])\n",
    "        #print(\"Validation accuracy: {}\".format(sum_/samples))\n",
    "    #print(\"Accuracy: {}\".format(sum_/samples))\n",
    "        \n",
    "    print('Epoch {} Loss {}, Accuracy {}'.format(epoch, loss.item(), acc/len(data_loader)))\n",
    "    print(\"---The runing time is: %s seconds ---\" % (time.time() - start_time))       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(seq2seq.state_dict(), 'C:\\\\Users\\A373503\\\\Desktop\\\\HDFS_04_11_deeplogmethod_finished\\\\seq2seq')\n",
    "torch.save(seq2seq.state_dict(), 'C:\\\\Users\\A373503\\\\Desktop\\\\HDFS_04_11_deeplogmethod_finished\\\\seq2seq_entire_testdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:\\\\Users\\A373503\\\\Desktop\\\\HDFS_04_11_deeplogmethod_finished\\\\pytorch\\\\seq2seq_entire_testdata')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_pickle('C:\\\\Users\\\\A373503\\\\Desktop\\\\Complete_4_models\\\\HDFS\\\\Data\\\\HDFS_test_data_mix_training_pytorch.pkl')\n",
    "print(\"The size of the test data is:\",len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sequences_of_events(log_key_sequence, window_size, step):\n",
    "    XX=[]\n",
    "    #y=[]\n",
    "    for i in range(0, len(log_key_sequence) - window_size, step):\n",
    "        sentence = log_key_sequence[i:i + window_size]\n",
    "        next_word = log_key_sequence[i + window_size]\n",
    "        XX.append(sentence)\n",
    "        #y.append(next_word)\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdwK3bempsXd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TP=0\n",
    "FP=0\n",
    "TN=0\n",
    "FN=0\n",
    "accs=list()\n",
    "\n",
    "\n",
    "for s in range(len(df_test)):\n",
    "    \n",
    "    #print(\"check:\",s)\n",
    "    sequence=df_test.EventSequence.iloc[s]\n",
    "    label=int(df_test.labels.iloc[s])\n",
    "    file_input=[]\n",
    "    \n",
    "    file_input.extend(transform_sequences_of_events(sequence, 10, 1))\n",
    "        \n",
    "    file_input=pd.Series(file_input)\n",
    "    file_df=pd.DataFrame(file_input, columns=['Windows'])\n",
    "    file=file_df['Windows']\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        acc=0\n",
    "        for window in range(len(file)):\n",
    "            x_predict = torch.LongTensor(file.iloc[window]).view(1, -1).to(device) #.cuda()\n",
    "    \n",
    "            acc+=sum([a == b for a, b in zip(seq2seq.predict(x_predict),x_predict.tolist()[0])])/len(x_predict.tolist()[0])\n",
    "\n",
    " \n",
    "\n",
    "        accuracy=acc/len(file)\n",
    "        accs.append(accuracy)\n",
    "        #print(\"check:\",s,accuracy)\n",
    "            \n",
    "    \n",
    "    #dataloader_file=dataloader_(file, len(file))\n",
    "    \n",
    "    #for k in dataloader_file:\n",
    "    #    #print(k.size())\n",
    "    #    preds=model.evaluate(k, word2id['<sos>'])\n",
    "    #    #print(preds[:,1:].size())\n",
    "    #    accuracy=torch.sum(preds[:,1:]==k)/(len(file)*10)\n",
    "        #print('Accuracy {}, Label {}'.format(accuracy, label))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x=np.arange(1,len(df_test)+1)\n",
    "plt.xlabel('Each file')\n",
    "plt.ylabel('Accuency')\n",
    "plt.plot(x,accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "threshold=0.6\n",
    "for i in range(len(accs)):\n",
    "    accuracy=accs[i]\n",
    "    if accuracy<threshold and label==1: #anomaly prediction\n",
    "        TP+=1\n",
    "    elif accuracy<threshold and label==0:\n",
    "        FP+=1\n",
    "    elif accuracy >threshold and label ==1:\n",
    "        FN+=1\n",
    "    else:\n",
    "        TN+=1\n",
    "\n",
    "print(\"Threshold is:\",threshold)\n",
    "print(\"True positive (anomaly with anomaly prediction):\",TP)\n",
    "print(\"False positive (normal with anomaly prediction): \",FP)\n",
    "\n",
    "print(\"False negative (anomaly with normal prediction):\",FN)\n",
    "print(\"True negative (normal with normal prediction):\",TN)\n",
    "            \n",
    "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "f1_score=(2*recall*precision)/(recall+precision)\n",
    "\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"F1 score:\",f1_score)\n",
    "print(\"\\n\")\n",
    "print(\"---The runing time is: %s seconds ---\" % (time.time() - start_time))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
